<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Manisha Natarajan</title>

    <meta name="author" content="Manisha Natarajan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Manisha Natarajan
                </p>
                <p>I'm a Robotics PhD Student in the School of Interactive Computing at <a href="https://www.gatech.edu/">Georgia Tech</a>. I work in the <a href="https://www.matthewtancik.com/nerf">CORE Robotics Lab</a> advised by <a href="https://www.cc.gatech.edu/people/matthew-gombolay">Dr. Matthew Gombolay</a>.
                  Prior to starting my PhD, I completed my Masters in Electrical and Computer Engineering at Georgia Tech, and my Bachelors in Electrical and Electronics from M.S. Ramaiah Institute Technology, India.
                </p>
                <p>
                  I am passionate about advancing the state-of-the-art in robotics and AI and creating intelligent and interactive systems that can assist, understand, and collaborate with humans
                  across various domains and contexts.
                </p>
                <p style="text-align:center">
                  <a href="mailto:manisha.natarajan@cc.gatech.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Manisha_Natarajan_Resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=cPrGzAMAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/manishanatarajan/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ManishaNatarajan">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/manisha.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/manisha.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests lie at the intersection of Artificial Intelligence (AI) and Human-Robot Interaction (HRI),
                  with an emphasis on 1) developing models to improve the robots' understanding of humans, and 2) helping robots communicate
                  their objectives to improve human understanding. My research focuses on settings where both humans and robots
                  are suboptimal and explores how to leverage the strengths of humans and robots to optimize team performance.
                  <br/> <br/> Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



    <tr onmouseout="bayes_stop()" onmouseover="bayes_start()" bgcolor="#d9f2f6">
      <td style="padding:20px;width:25%;vertical-align:middle">
<!--        <div class="one">-->
<!--          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>-->
<!--          <source src="images/smerf.mp4" type="video/mp4">-->
<!--          Your browser does not support the video tag.-->
<!--          </video></div>-->
<!--          <img src='images/smerf.jpg' width=100%>-->
<!--        </div>-->
        <div class="one">
            <div class="two" id='bayes_image'>
              <img src='images/bayes.png' width="210" height="140"></div>
              <img src='images/bayes.png' width="210" height="140">
          </div>
        <script type="text/javascript">
          function bayes_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function bayes_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          bayes_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">Mixed-Initiative Human-Robot Teaming under Suboptimality with Online Bayesian Adaptation</span>
        </a>
        <br>
		<strong>Manisha Natarajan*</strong>,
		<a href="">Chunyue Xue*</a>,
		<a href="https://sannevw.github.io/">Sanne van Waveren</a>,
		<a href="">Karen Feigh</a>,
		<a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
        <br>
        <a href="https://www.aamas2024-conference.auckland.ac.nz/">AAMAS</a>, 2024
        <br>
        <br>
        <a href="https://arxiv.org/abs/2403.16178">arXiv</a>
        /
        <a href="https://github.com/CORE-Robotics-Lab/Bayes-POMCP">code</a>
        <p></p>
        <p>
        We adopt an online Bayesian approach that enables a robot to infer people's willingness to comply with its
          assistance in a sequential decision-making game.
        </p>
      </td>
    </tr>
	

    <tr onmouseout="cadence_stop()" onmouseover="cadence_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
<!--        <div class="one">-->
<!--          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>-->
<!--          <source src="images/nuvo.mp4" type="video/mp4">-->
<!--          Your browser does not support the video tag.-->
<!--          </video></div>-->
<!--          <img src='images/nuvo.jpg' width=100%>-->
<!--        </div>-->
        <div class="one">
            <div class="two" id='cadence_image'>
              <img src='images/cadence.png' width="210" height="120"></div>
              <img src='images/cadence.png' width="210" height="120">
          </div>
        <script type="text/javascript">
          function cadence_start() {
            document.getElementById('cadence_image').style.opacity = "1";
          }

          function cadence_stop() {
            document.getElementById('cadence_image').style.opacity = "0";
          }
          cadence_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2307.06244.pdf">
          <span class="papertitle">Diffusion Models for Multi-target Adversarial Tracking</span>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/sean-ye-a64334a4/">Sean Ye</a>,
        <strong>Manisha Natarajan</strong>,
        <a href="">Zixuan Wu</a>,
		<a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
        <br>
        <a href="https://sites.bu.edu/mrs2023/">IEEE MRS</a>, 2023
        <br>
        <br>
        <a href="https://arxiv.org/pdf/2307.06244.pdf">arXiv</a>

        <p></p>
        <p>
        We present CADENCE, a diffusion model aimed at generating comprehensive predictions of multiple adversary
          locations by leveraging past sparse state information.
        </p>
      </td>
    </tr>
	
	
    <tr onmouseout="marl_stop()" onmouseover="marl_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
<!--        <div class="one">-->
<!--          <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>-->
<!--          <source src="images/recon.mp4" type="video/mp4">-->
<!--          Your browser does not support the video tag.-->
<!--          </video></div>-->
<!--          <img src='images/recon.png' width="160">-->
<!--        </div>-->
          <div class="one">
            <div class="two" id='marl_image'>
              <img src='images/marl.gif' width="160"></div>
              <img src='images/marl.gif' width="160">
          </div>
        <script type="text/javascript">
          function marl_start() {
            document.getElementById('marl_image').style.opacity = "1";
          }

          function marl_stop() {
            document.getElementById('marl_image').style.opacity = "0";
          }
          marl_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2306.11301.pdf">
			<span class="papertitle">Adversarial Search and Tracking with Multi-Agent Reinforcement Learning in a Sparsely Observable Environment</span>
        </a>
        <br>
        <a href="">Zixuan Wu*</a>,
		<a href="https://www.linkedin.com/in/sean-ye-a64334a4/">Sean Ye*</a>,
        <strong>Manisha Natarajan</strong>,
        <a href="http://www.letianchen.me/">Letian Chen</a>,
        <a href="https://www.rohanpaleja.com/">Rohan Paleja</a>,
        <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
        <br>
        <a href="https://sites.bu.edu/mrs2023/">IEEE MRS</a>, 2023
        <br>
        <br>
        <a href="https://arxiv.org/pdf/2306.11301.pdf">arXiv</a>
        <p></p>
        <p>
        We propose a novel Multi-Agent Reinforcement Learning framework that leverages the estimated state location
          of an opponent from a filtering pipeline to produce interdiction paths for a team of tracking agents in pursuit-evasion domains.
        </p>
      </td>
    </tr>


    <tr onmouseout="challenges_stop()" onmouseover="challenges_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='challenges_image'>
              <img src='images/challenges.png' width="160"></div>
              <img src='images/challenges.png' width="160">
          </div>
          <script type="text/javascript">
            function challenges_start() {
              document.getElementById('challenges_image').style.opacity = "1";
            }

            function challenges_stop() {
              document.getElementById('challenges_image').style.opacity = "0";
            }
            challenges_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://link.springer.com/article/10.1007/s43154-023-00103-1">
            <span class="papertitle">Human-Robot Teaming: Grand Challenges</span>
          </a>
          <br>
          <strong>Manisha Natarajan*</strong>,
          <a href="">Esmaeil Seraj*</a>,
          <a href="">Batuhan Altundas*</a>,
          <a href="https://www.rohanpaleja.com/">Rohan Paleja*</a>,
          <a href="https://www.linkedin.com/in/sean-ye-a64334a4/">Sean Ye*</a>,
          <a href="http://www.letianchen.me/">Letian Chen*</a>,
          <a href="">Reed Jensen</a>,
          <a href="">Kimberly Chestnut Chang</a>,
          <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
          <br>
          <em>Current Robotics Reports</em>, 2023
          <br>
          <br>
          <p></p>
          <p>
          We review the field of Human-Robot teaming (HRT) and identify key challenges to guide the research community
            towards successful HRT while avoiding potential pitfalls.
          </p>
        </td>
      </tr>


    <tr onmouseout="grammi_stop()" onmouseover="grammi_start()" bgcolor="#d9f2f6">
  <td style="padding:20px;width:25%;vertical-align:middle">
<!--    <div class="one">-->
<!--    <div class="two" id='difsurvey_image'><video  width=100% height=100% muted autoplay loop>-->
<!--    <source src="images/difsurvey_video.mp4" type="video/mp4">-->
<!--    Your browser does not support the video tag.-->
<!--    </video></div>-->
<!--      <img src='images/difsurvey_image.jpg' width="160">-->
<!--    </div>-->
      <div class="one">
            <div class="two" id='grammi_image'>
              <img src='images/grammi.png' width="210" height="140"></div>
              <img src='images/grammi.png' width="210" height="140">
          </div>
    <script type="text/javascript">
      function grammi_start() {
        document.getElementById('grammi_image').style.opacity = "1";
      }

      function grammi_stop() {
        document.getElementById('grammi_image').style.opacity = "0";
      }
      grammi_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2310.07204">
      <span class="papertitle">Learning Models of Adversarial Agent Behavior under Partial Observability</span>
    </a>
    <br>
	<a href="https://www.linkedin.com/in/sean-ye-a64334a4/">Sean Ye*</a>,
    <strong>Manisha Natarajan*</strong>,
    <a href="">Zixuan Wu*</a>,
    <a href="https://www.rohanpaleja.com/">Rohan Paleja</a>,
    <a href="http://www.letianchen.me/">Letian Chen</a>,
    <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
    <br>
    <a href="https://ieee-iros.org/">IROS</a>, 2023
    <br>
    <br>
    <a href="https://arxiv.org/pdf/2306.11168.pdf">arXiv</a> /
    <a href="https://github.com/CORE-Robotics-Lab/Opponent-Modeling">code</a> /
    <a href="https://youtu.be/HPAARwe_CE8">video</a>
    <p></p>
    <p>
    We present a novel architecture that uses Graph Neural Networks with a Mutual Information formalism to predict the current
      and future states of an adversarial opponent in large-scale pursuit-evasion domains.
    </p>
  </td>
</tr>          


    <tr onmouseout="likert_stop()" onmouseover="likert_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
<!--        <div class="one">-->
<!--          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>-->
<!--          <source src="images/camp.mp4" type="video/mp4">-->
<!--          Your browser does not support the video tag.-->
<!--          </video></div>-->
<!--          <img src='images/camp.png' width="160">-->
<!--        </div>-->
        <div class="one">
            <div class="two" id='likert_image'>
              <img src='images/likert.png' width="210" height="140"></div>
              <img src='images/likert.png' width="210" height="140">
          </div>
        <script type="text/javascript">
          function likert_start() {
            document.getElementById('likert_image').style.opacity = "1";
          }

          function likert_stop() {
            document.getElementById('likert_image').style.opacity = "0";
          }
          likert_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3572784">
          <span class="papertitle">Concerning Trends in Likert Scale Usage in Human-Robot Interaction:
            Towards Improving Best Practices</span>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/mariah-schrum-531972156/">Mariah Schrum</a>,
        <a href="https://www.linkedin.com/in/muyleng-ghuy/">Muyleng Ghuy</a>,
        <a href="https://bmild.github.io/">Erin Hedlund-Botti</a>,
        <strong>Manisha Natarajan</strong>,
        <a href="https://www.linkedin.com/in/michael-johnson-gatech/">Michael Johnson</a>,
        <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
        <br>
        <em>ACM Transactions on Human-Robot Interaction (THRI)</em>, 2023
        <br>
        <br>
        <p></p>
        <p>
        We report the incorrect statistical practices in the field of HRI (for papers published through 2016 - 2020)
          and conducted a survey of best practices across several venues to provide a comparative analysis on how
          Likert practices differ across the field of HRI.
        </p>
      </td>
    </tr>


    <tr onmouseout="attitude_stop()" onmouseover="attitude_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
<!--          <div class="one">-->
<!--            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>-->
<!--            <source src="images/zipnerf.mp4" type="video/mp4">-->
<!--            Your browser does not support the video tag.-->
<!--            </video></div>-->
<!--            <img src='images/zipnerf.jpg' width="160">-->
<!--          </div>-->
          <div class="one">
            <div class="two" id='attitude_image'>
              <img src='images/attitudes.png' width="210" height="160"></div>
              <img src='images/attitudes.png' width="210" height="160">
          </div>
          <script type="text/javascript">
            function attitude_start() {
              document.getElementById('attitude_image').style.opacity = "1";
            }

            function attitude_stop() {
              document.getElementById('attitude_image').style.opacity = "0";
            }
            attitude_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/pdf/10.1145/3568162.3576996">
            <span class="papertitle">Impacts of Robot Learning on User Attitude and Behavior</span>
          </a>
          <br>
          <a href="https://www.ninamoorman.com/">Nina Moorman</a>,
          <a href="https://bmild.github.io/">Erin Hedlund-Botti</a>,
          <a href="https://www.linkedin.com/in/mariah-schrum-531972156/">Mariah Schrum</a>,
          <strong>Manisha Natarajan</strong>,
          <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
          <br>
          <a href="">HRI</a>, 2023
          <br>
          <br>
          <p></p>
          <p>
          We examine how different learning methods (e.g., reinforcement learning, learning from demonstrations, interactive learning)
            influence the users perceptions of an in-home assistive robot.
          </p>
        </td>
      </tr>
      
      
    <tr onmouseout="driving_stop()" onmouseover="driving_start()" bgcolor="#d9f2f6">
        <td style="padding:20px;width:25%;vertical-align:middle">
<!--          <div class="one">-->
<!--            <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>-->
<!--            <source src="images/owl.mp4" type="video/mp4">-->
<!--            Your browser does not support the video tag.-->
<!--            </video></div>-->
<!--            <img src='images/owl.png' width="160">-->
<!--          </div>-->
          <div class="one">
            <div class="two" id='driving_image'>
              <img src='images/driving.png' width="210" height="140"></div>
              <img src='images/driving.png' width="210" height="140">
          </div>
          <script type="text/javascript">
            function driving_start() {
              document.getElementById('driving_image').style.opacity = "1";
            }

            function driving_stop() {
              document.getElementById('driving_image').style.opacity = "0";
            }
            driving_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">Towards Adaptive Driving Styles for Automated Driving with Users'
            Trust and Preferences</span>
          </a>
          <br>
          
          <strong>Manisha Natarajan</strong>,
          <a href="">Kumar Akash</a>,
          <a href="">Teruhisa Misu</a>
          <br>
          <a href="">HRI - Late Breaking Report</a>, 2022
          <br>
          <br>
          <a href="https://youtu.be/2YBjUoZbaDg">video</a>
          <p></p>
          <p>We explore different methods to adapt the driving style of an autonomous vehicle to match the preferred driving
          styles of users and improve their trust in the vehicle.</p>
        </td>
      </tr>


    <tr onmouseout="negative_stop()" onmouseover="negative_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
<!--          <div class="one">-->
<!--            <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>-->
<!--            <source src="images/bakedsdf_after.mp4" type="video/mp4">-->
<!--            Your browser does not support the video tag.-->
<!--            </video></div>-->
<!--            <img src='images/bakedsdf_before.jpg' width="160">-->
<!--          </div>-->
          <div class="one">
            <div class="two" id='negative_image'>
              <img src='images/negative.png' width="210" height="140"></div>
              <img src='images/negative.png' width="210" height="140">
          </div>

          <script type="text/javascript">
            function negative_start() {
              document.getElementById('negative_image').style.opacity = "1";
            }

            function negative_stop() {
              document.getElementById('negative_image').style.opacity = "0";
            }
            negative_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">Negative Result for Learning from Demonstration: Challenges
            for End-Users Teaching Robots with Task and Motion Planning Abstractions</span>
          </a>
          <br>
          <a href="https://search.asu.edu/profile/4293865">Nakul Gopalan</a>,
          <a href="https://www.ninamoorman.com/">Nina Moorman</a>,
          <strong>Manisha Natarajan</strong>,
          <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
          <br>
          <a href="">RSS</a>, 2022
          <br>
          <br>
          <p></p>
          <p>
          We conduct two novel human-subjects experiments to determine what instructional information is required
            to support uses with non-robotics experience to learn to program robots effectively to solve novel tasks via demonstrations.
          </p>
        </td>
      </tr>


    <tr onmouseout="schedule_stop()" onmouseover="schedule_start()" bgcolor="#d9f2f6">
        <td style="padding:20px;width:25%;vertical-align:middle">
<!--          <div class="one">-->
<!--            <div class="two" id='merf_image'><video  width=100% height=100% muted autoplay loop>-->
<!--            <source src="images/merf_after.mp4" type="video/mp4">-->
<!--            Your browser does not support the video tag.-->
<!--            </video></div>-->
<!--            <img src='images/merf_before.jpg' width="160">-->
<!--          </div>-->
          <div class="one">
            <div class="two" id='schedule_image'>
              <img src='images/schedule.png' width="210" height="140"></div>
              <img src='images/schedule.png' width="210" height="140">
          </div>
          <script type="text/javascript">
            function schedule_start() {
              document.getElementById('schedule_image').style.opacity = "1";
            }

            function schedule_stop() {
              document.getElementById('schedule_image').style.opacity = "0";
            }
            schedule_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/pdf/10.1145/3477391" >
            <span class="papertitle">Coordinating Human-Robot Teams with Dynamic and Stochastic Task Proficiencies</span>
          </a>
          <br>
          <a href="https://www.linkedin.com/in/eric-liu-93356b42/">Ruisen Liu*</a>,
          <strong>Manisha Natarajan*</strong>,
          <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
          <br>
          <em>ACM Transactions on Human-Robot Interaction (THRI)</em>, 2023
          <br>
          <br>
            <a href="https://youtu.be/2ICiWZsZ_7A">video</a>
          <p></p>
          <p>
          We introduce a novel resource coordination algorithm that enables robots to schedule team activities by predicting
            the task performance of their human teammates while ensuring that the schedule is robust to temporal constraints.
          </p>
        </td>
      </tr>


    <tr onmouseout="trust_stop()" onmouseover="trust_start()" bgcolor="#d9f2f6">
        <td style="padding:20px;width:25%;vertical-align:middle">
<!--          <div class="one">-->
<!--            <div class="two" id='trust_image'><video  width=100% height=100% muted autoplay loop>-->
<!--            <source src="images/trust.mp4" type="video/mp4">-->
<!--            Your browser does not support the video tag.-->
<!--            </video></div>-->
<!--            <img src='images/trust.webp' width="160">-->
<!--          </div>-->
          <div class="one">
            <div class="two" id='trust_image'>
              <img src='images/trust.webp' width="210" height="140"></div>
              <img src='images/trust.webp' width="210" height="140">
          </div>
          <script type="text/javascript">
            function trust_start() {
              document.getElementById('trust_image').style.opacity = "1";
            }

            function trust_stop() {
              document.getElementById('trust_image').style.opacity = "0";
            }
            trust_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="">
            <span class="papertitle">Effects of Anthropomorphism and Accountability on Trust in Human-Robot Interaction</span>
          </a>
          <br>
          <strong>Manisha Natarajan</strong>,
          <a href="https://sites.gatech.edu/matthew-gombolay/">Matthew Gombolay</a>
          <br>
          <a href="">HRI</a>, 2020
          <br>
          <br>
          <a href="https://youtu.be/An-4AnTJ04M">video</a>
          <p></p>
          <p>
          We conducted a human-subjects experiment to examine how people's trust and dependence on robot teammates providing
            decision support varies as a function of different attributes of the robot, such as perceived anthropomorphism,
            type of support provided by the robot, and its physical presence.
          </p>
        </td>
      </tr>
    


    </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <br>
                <br>
                <br>
                <h2>Previous Internships</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img width=125px src="images/briefcase.png"></td>
              <td width="75%" valign="center">
                <a href="https://usa.honda-ri.com/">Summer Intern, <strong>Honda Research Institute</strong>, May - August, 2021</a>
                <br>
                <a href="https://r-dex.com/">Robotics Intern, <strong>R-DEX Systems</strong>, August - December, 2018</a>
                <br>
                <a href="https://www.magicleap.com/">Machine Vision Intern, <strong>Magic Leap</strong>, May - August, 2018</a>
                <br>
                <a href="https://www.iitb.ac.in/">Summer Research Fellow, <strong>Indian Institute of Technology - Bombay</strong>, May - August, 2016</a>
                <br>
                <a href="https://www.bhel.com/">Electrical Engineering Intern, <strong>Bharat Heavy Electricals Limited</strong>, May - August, 2015</a>
              </td>
            </tr>
            </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <br>
                <h2>Teaching Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img width=125px src="images/teach.png" alt="teach">
              </td>
              <td width="75%" valign="center">
                <a href="">Graduate Teaching Assistant, CS7648: Interactive Robot Learning, Spring 2024</a>
                <br>
                <a href="">Graduate Teaching Assistant, CS7649: Robot Intelligence Planning, Fall 2020</a>
                <br>
                <a href="">Graduate Teaching Assistant, CS7641: Machine Learning, Spring 2019</a>
              </td>
            </tr>
            </tbody></table>
            


            
            

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:left;font-size:small;">
                  Thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the website template [<a href="https://github.com/jonbarron/jonbarron_website">Source code</a>].
                  Icons taken from <a href="https://www.flaticon.com/">flaticon.com</a>
                  <br>
                  Last Updated: March, 2024.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
